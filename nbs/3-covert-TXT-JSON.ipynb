{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "import json\n",
    "import glob\n",
    "from tqdm.notebook import trange,tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B-LOCATION\n",
    "# B-ORGANIZATION\n",
    "# B-PERSON\n",
    "# I-LOCATION\n",
    "# I-ORGANIZATION\n",
    "# I-PERSON\n",
    "# O\n",
    "labels_dict = {\n",
    "    \"O\":0,\n",
    "    \"B-PERSON\":1,\n",
    "    \"I-PERSON\":2,\n",
    "    \"B-LOCATION\":3,\n",
    "    \"I-LOCATION\":4,\n",
    "    \"B-ORGANIZATION\":5,\n",
    "    \"I-ORGANIZATION\":6,\n",
    "    \"B-FESTIVAL\":7,\n",
    "    \"I-FESTIVAL\":8,\n",
    "    \"B-GAME\":9,\n",
    "    \"I-GAME\":10,\n",
    "    \"B-LANGUAGE\":11,\n",
    "    \"I-LANGUAGE\":12,\n",
    "    \"B-LITERATURE\":13,\n",
    "    \"I-LITERATURE\":14,\n",
    "    \"B-MISC\":15,\n",
    "    \"I-MISC\":16,\n",
    "    \"B-NUMEX\":17,\n",
    "    \"I-NUMEX\":18,\n",
    "    \"B-RELIGION\":19,\n",
    "    \"I-RELIGION\":20,\n",
    "    \"B-TIMEX\":21,\n",
    "    \"I-TIMEX\":22,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_dict = {\n",
    "#     \"ADJ\":0,\n",
    "#     \"ADP\":1,\n",
    "#     \"ADV\":2,\n",
    "#     \"AUX\":3,\n",
    "#     \"CONJ\":4,\n",
    "#     \"CCONJ\":5,\n",
    "#     \"DET\":6,\n",
    "#     \"INTJ\":7,\n",
    "#     \"NOUN\":8,\n",
    "#     \"NUM\":9,\n",
    "#     \"PART\":10,\n",
    "#     \"PRON\":11,\n",
    "#     \"PROPN\":12,\n",
    "#     \"PUNCT\":13,\n",
    "#     \"SCONJ\":14,\n",
    "#     \"SYM\":15,\n",
    "#     \"VERB\":16,\n",
    "#     \"X\":17,\n",
    "#     \"SPACE\":18\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(filepath):\n",
    "    #logger.info(\"‚è≥ Generating examples from = %s\", filepath)\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        guid = 0\n",
    "        tokens = []\n",
    "        pos_tags = []\n",
    "        #chunk_tags = []\n",
    "        ner_tags = []\n",
    "        final = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                if tokens:\n",
    "                    yield_dict = {\n",
    "                        \"id\": str(guid),\n",
    "                        \"tokens\": tokens,\n",
    "                        #\"pos_tags\": pos_tags,\n",
    "                        \"ner_tags\": ner_tags\n",
    "                    }\n",
    "                    final.append(yield_dict)\n",
    "                    guid += 1\n",
    "                    tokens = []\n",
    "                    #pos_tags = []\n",
    "                    #chunk_tags = []\n",
    "                    ner_tags = []\n",
    "            else:\n",
    "                # conll2003 tokens are space separated\n",
    "                splits = line.split(\"\\t\")\n",
    "                tokens.append(splits[0].strip())\n",
    "                #pos_tags.append(pos_dict[splits[1].strip()])\n",
    "                #chunk_tags.append(splits[2])\n",
    "                ner_tags.append(labels_dict[splits[1].strip()])\n",
    "        # last example\n",
    "        yield_dict= {\n",
    "            \"id\": str(guid),\n",
    "            \"tokens\": tokens,\n",
    "            #\"pos_tags\": pos_tags,\n",
    "            #\"chunk_tags\": chunk_tags,\n",
    "            \"ner_tags\": ner_tags,\n",
    "        }\n",
    "        #final.append(yield_dict)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = generate_examples(\"./data/collapsed/train.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/collapsed/train.json', 'w') as fp:\n",
    "    json.dump(data_, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"./data/*-bio\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.split(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./data/sdu_data_trunc.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ = file\n",
    "data_ = generate_examples(file_)\n",
    "\n",
    "op_name = \"./processed/json/\" + file_.split(\"/\")[2] + \".json\"\n",
    "\n",
    "with open(op_name, w) as fp:\n",
    "    json.dump(data_, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(6):\n",
    "    file_ = files[i]\n",
    "    data_ = generate_examples(file_)\n",
    "    \n",
    "    op_name = \"./processed/json/\" + file_.split(\"/\")[2] + \".json\"\n",
    "    \n",
    "    with open(op_name, 'w') as fp:\n",
    "        json.dump(data_, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../HF-HiNER-collapsed/data/train_clean.json', 'r') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[]\n",
    "tags=[]\n",
    "with open(\"data/test_clean.conll\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"\\n\"):\n",
    "            continue\n",
    "        else:\n",
    "            line = line.split(\"\\t\")\n",
    "            if(line[0] == \"\" or line[0] == \" \" or line[0].strip() == \"\"):\n",
    "                print(\"token issue\")\n",
    "                break\n",
    "            else:\n",
    "                tokens.append(line[0])\n",
    "            if(line[1] == \"\" or line[1] == \" \" or line[1].strip() == \"\"):\n",
    "                print(\"tag issue\")\n",
    "                break\n",
    "            else:\n",
    "                tags.append(line[1].rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens), len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagset = list(set(tags))\n",
    "vocab = list(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445de843e19845ccb4088bdc8f8c4d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset hi_ner_collapsed_config/HiNER-Collapsed (download: 68.27 MiB, generated: 51.05 MiB, post-processed: Unknown size, total: 119.32 MiB) to /home/diptesh/.cache/huggingface/datasets/cfilt___hi_ner_collapsed_config/HiNER-Collapsed/0.0.2/6f6e240c6bd7b047cc113c6e39be72c86e917740503cc0315acd869715a13efc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26bea400a1449298137f9c3095f1a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f3efb5423042e68edd6b7af4e1f6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/47.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f08a95a66c145eeb2eafb563ebe45f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761e72561b944e3995e3811360c53fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed52d0f00a2b4020bffb5e98d0cd8609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77decdd9e3fa46ea981fb592f6376b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/76025 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d6f54b3bb5418a8ccffeb1d51531d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad4b9f7a9c3448eb5bb22710273378a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/21722 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset hi_ner_collapsed_config downloaded and prepared to /home/diptesh/.cache/huggingface/datasets/cfilt___hi_ner_collapsed_config/HiNER-Collapsed/0.0.2/6f6e240c6bd7b047cc113c6e39be72c86e917740503cc0315acd869715a13efc. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a03936a7ca7400e87e4e9a30d105145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "hiner = load_dataset(\"cfilt/HiNER-collapsed\", download_mode='force_redownload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 76025\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 10861\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 21722\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(hiner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§∏‡•ã‡§®‡•Ä ‡§∏‡•ç‡§ü‡•Ç‡§°‡§ø‡§Ø‡•ã ‡§ï‡§æ ‡§ï‡§π‡§®‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ú‡•á‡§Æ‡•ç‡§∏ ‡§¨‡§æ‡•ç‡§° O ‡§∏‡§ø‡§∞‡•Ä‡§ú‡§º ‡§ï‡•Ä ‡§®‡§à ‡§´‡§º‡§ø‡§≤‡•ç‡§Æ ‡§®‡•á ‡§∞‡§ø‡§≤‡•Ä‡§ú‡§º ‡§ï‡•á ‡§™‡§π‡§≤‡•á ‡§π‡•Ä ‡§¶‡§ø‡§® ‡§¨‡•â‡§ï‡•ç‡§∏ ‡§ë‡§´‡§ø‡§∏ ‡§™‡§∞ ‡§®‡§Ø‡§æ ‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§° ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§π‡•à O\n"
     ]
    }
   ],
   "source": [
    "# print(hiner['test'][8])\n",
    "# print(hiner['test'][16])\n",
    "# print(hiner['test'][24])\n",
    "# print(hiner['test'][25])\n",
    "print(' '.join(hiner['test'][35]['tokens'])) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd5003afe4d304b87f77d601a74a130b1881051bb00fb0d8527146eb7f967d2e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('hfdataset')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
